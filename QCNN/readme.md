MNIST Classification with quantum circuits

Origin: https://github.com/PennyLaneAI/qml/blob/master/demonstrations/tutorial_quanvolution.py

Classical convolution

The convolutional neural network (CNN) is a standard model in classical machine learning which is particularly suitable for processing images. The model is based on the idea of a convolution layer where, instead of processing the full input data with a global function, a local convolution is applied. If the input is an image, small local regions are sequentially processed with the same kernel. The results obtained for each region are usually associated to different channels of a single output pixel. The union of all the output pixels produces a new image-like object, which can be further processed by additional layers.

Quantum convolution

One can extend the same idea also to the context of quantum variational circuits. A small region of the input image is embedded into a quantum circuit. In this demo, this is achieved with parametrized rotations applied to the qubits initialized in the ground state. A quantum computation, associated to a unitary is performed on the system. The unitary could be generated by a variational quantum circuit or, more simply, by a random circuit as proposed in Ref. [1]. The quantum system is finally measured, obtaining a list of classical expectation values. The measurement results could also be classically post-processed as proposed in Ref. [1] but, for simplicity, in this demo we directly use the raw expectation values. Analogously to a classical convolution layer, each expectation value is mapped to a different channel of a single output pixel. Iterating the same procedure over different regions, one can scan the full input image, producing an output object which will be structured as a multi-channel image. The quantum convolution can be followed by further quantum layers or by classical layers. The main difference with respect to a classical convolution is that a quantum circuit can generate highly complex kernels whose computation could be, at least in principle, classically intractable.

Results:

1. n_train = 50, n_test = 30
![](https://raw.githubusercontent.com/aprameyap/Quantum-Notebooks/main/QCNN/50_30.png)

2. n_train = 100, n_test = 50
![](https://raw.githubusercontent.com/aprameyap/Quantum-Notebooks/main/QCNN/100_50.png)

3. n_train = 1000, n_test = 500
![](https://raw.githubusercontent.com/aprameyap/Quantum-Notebooks/main/QCNN/1000_500.png)

As the training set grows, accuracy of qcnn becomes greater than cnn. This shows that a hybrid model reduces the overall trainable parameters.

Reference: [1] Maxwell Henderson, Samriddhi Shakya, Shashindra Pradhan, Tristan Cook. “Quanvolutional Neural Networks: Powering Image Recognition with Quantum Circuits.” arXiv:1904.04767, 2019.
